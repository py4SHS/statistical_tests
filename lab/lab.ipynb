{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to statistical testing\n",
    "\n",
    "This afternoon we will focus on the application of Student and Welsh's test on Pauline epistles, in order to assess possible different use of conjunctions that would indicate a **different authorship** between the epistle to the Colossians and \"authentic\" epistles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements for the lab that you should have installed in your virtual environment:\n",
    "- `scipy`\n",
    "- `pandas`\n",
    "- `scikit-learn`\n",
    "- `seaborn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "In the dataset `pauline.csv` you will find a three columns dataset containing the **lemmatized** text of the authentic epistles and Colossians split over a 50 token frequency. The dataset has three columns: `book`, `chunk` (corresponds to the 50 tokens text chunk), and `text`.\n",
    "\n",
    "The goal of the pre-processing is now to structure the dataset in order to have the following column:\n",
    "`book`, `chunk` (corresponds to the 50 tokens text chunk) and `functional_words_frequency`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The list of words that can be considered to be functional words is stored in the list `STOP_WORDS`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set([\n",
    "    \"ἄλλος\", \"ἄν\", \"ἄρα\", \"ἀλλ'\", \"ἀλλά\", \"ἀπό\", \"αὐτός\", \"δ'\", \"δαί\", \"δαίς\", \"δέ\", \"δή\",\n",
    "    \"διά\", \"ἑαυτοῦ\", \"ἔτι\", \"ἐάν\", \"ἐγώ\", \"ἐκ\", \"ἐμός\", \"ἐν\", \"ἐπί\", \"εἰ\", \"εἰμί\", \"εἶμι\",\n",
    "    \"εἰς\", \"γάρ\", \"γὰ\", \"γε\", \"ἡ\", \"ἦ\", \"καί\", \"κατά\", \"μέν\", \"μετά\", \"μή\", \"ὁ\", \"ὅδε\",\n",
    "    \"ὅς\", \"ὅστις\", \"ὅτι\", \"οἱ\", \"οὕτως\", \"οὗτος\", \"οὐ\", \"οὔτε\", \"οὖν\", \"οὐδέ\", \"οὐδείς\",\n",
    "    \"οὐκ\", \"παρά\", \"περί\", \"πρός\", \"σός\", \"σύ\", \"σύν\", \"τά\", \"τε\", \"τήν\", \"τῆς\", \"τῇ\",\n",
    "    \"τί\", \"τί\", \"τίς\", \"τις\", \"τό\", \"τόν\", \"τοί\", \"τοιοῦτος\", \"τούς\", \"τοῦ\", \"τῶν\", \"τῷ\",\n",
    "    \"ὑμός\", \"ὑπέρ\", \"ὑπό\", \"ὥστε\", \"ὡς\", \"ὦ\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**:\n",
    "1. Load the dataset in `pauline.csv` into a pandas DataFrame `pauline`.\n",
    "2. Compute on the column `text` the word counts of the words contained in the list `STOP_WORDS` (hint: use the parameter `vocabulary` of the class `CountVectorizer` that we used on Day 3).\n",
    "3. Sum these words frequencies in order to have the global functional word frequencies and store them into a variable containing numpy arrays `word_freq`.\n",
    "4. Using the method `assign`, add a new column `functional_freq` to the DataFrame `pauline`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing statistical tests, we can check visually and statistically the distribution of the data, using the skills that we developed during day 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**:\n",
    "1. Using `pandas` filter, create a new variable called `is_col`, that equals to 1 if the chunk corresponds to the book `Col` and 0 otherwise.\n",
    "2. Assign using the method `assign`, add this new variable to the dataset `pauline` (you can for example call it `is_col`).\n",
    "3. Give the major statistical estimators (mean, median, standard error) for functional word frequencies across Colossians and the authentic epistles.\n",
    "4. Plot the distribution of functional word frequencies across Colossians and the authentic epistles using the adequate graph for this data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing statistical tests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student tests are available in `scipy` using the `ttest_ind` from the module `stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It provides several options (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html#scipy.stats.ttest_ind), the most important for us being the `equal_var` parameter, that when sets to `True` performs a standard Student test and set to `False` a Welsch test. In this lab, we will go for the Welsch test as we detailed it lecture.\n",
    "It outputs the `t_stat` as well as the `p_val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using randomly generated artificial samples\n",
    "import numpy as np\n",
    "\n",
    "# Generate two samples to test\n",
    "sample_1 = np.random.normal(0, 1, 1000)\n",
    "sample_2 = np.random.normal(1, 1, 1000)\n",
    "\n",
    "# Perform the t-test\n",
    "t_stat, p_val = ttest_ind(sample_1, sample_2)\n",
    "\n",
    "# Can you conclude regarding a different distribution ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**\n",
    "1. Using this new variable `is_col`, extract as two `numpy` arrays the frequencies for the epistle of the Colossian and for the authentic epistles (using the attribute `.values` of pandas Series).\n",
    "2. Perform the Student's test on the two arrays.\n",
    "3. Conclude regarding different use of functional words in Colossian and authentic episles.\n",
    "4. **Bonus**: Test the normality hypothesis using Shapiro's test (`scipy.stats.shapiro`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: linear regression and statistical testing \n",
    "\n",
    "Go to `lab_answers.ipynb `"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
